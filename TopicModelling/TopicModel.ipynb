{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### College of Computing and Informatics, Drexel University\n",
    "### INFO 323: Cloud Computing and Big Data\n",
    "### Due: Tuesday, June 13, 2023\n",
    "---\n",
    "\n",
    "## Final Project Report\n",
    "\n",
    "## Project Title: Unsupervised Clustering of Wikipedia Articles\n",
    "\n",
    "## Student(s): Charlie Gormley; Monish B.\n",
    "\n",
    "#### Date: Tuesday, June 13, 2023\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project revolves around performing unsupervised clustering with a large dataset of Wikipedia articles. The model we use to perform this clustering is Latent Dirichlet Allocation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data comes from a majority of Wikipedia articles during the year 2017→2018. The total database is ~4.5 million articles and 20 GB of data. We decided to just use a subset of randomly selected 500,000 articles (3 GB) due to to computational resources neseccary to train models with 4.5 million articles. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    " For our EDA process we created a removed stop words and transformed each article of text into TFIDF. We also created an elbow plot to estimate the optimal number of clusters for our dataset. We computed TFIDF through first putting each article through a tokenizer which converts of the words into an element in a list. Then we iterate through those lists and remove any stopwords. After this we vectorize each article through counting the occurrence of each word. Then we put the vectorized counts through idf. We then will use this TFIDF transformation as our input vector for LDA-Clustering. \n",
    "\n",
    "Also with EDA we utilized, as viewed below, silhouetee scores to estimate the prowess of a given k-value. We saw better scores with lower k-values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling & Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For modeling we tune our LDA through differing levels of k and compare each model with a different k-value with the model’s silhouette-value, and through the elbow analysis\n",
    "\n",
    "We then pick the best model and run it again to analyze the topics with the *analyze_topics* function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTEBOOK**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SetUp\n",
    "Importing & Device Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\INFO323\\\\TokenizedToast'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading In Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = 'Data\\enwiki-20170820.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(select, db=db):\n",
    "    '''\n",
    "    1. Connects to SQLite database (db)\n",
    "    2. Executes select statement\n",
    "    3. Return results and column names\n",
    "    \n",
    "    Input: 'select * from analytics limit 2'\n",
    "    Output: ([(1, 2, 3)], ['col_1', 'col_2', 'col_3'])\n",
    "    '''\n",
    "    with sql.connect(db) as conn:\n",
    "        c = conn.cursor()\n",
    "        c.execute(select)\n",
    "        col_names = [str(name[0]).lower() for name in c.description]\n",
    "    return c.fetchall(), col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, lower=True):\n",
    "    '''\n",
    "    1. Strips apostrophes\n",
    "    2. Searches for all alpha tokens (exception for underscore)\n",
    "    3. Return list of tokens\n",
    "\n",
    "    Input: 'The 3 dogs jumped over Scott's tent!'\n",
    "    Output: ['the', 'dogs', 'jumped', 'over', 'scotts', 'tent']\n",
    "    '''\n",
    "    text = re.sub(\"'\", \"\", text)\n",
    "    if lower:\n",
    "        tokens = re.findall('''[a-z_]+''', text.lower())\n",
    "    else:\n",
    "        tokens = re.findall('''[A-Za-z_]''', text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(article_id):\n",
    "    '''\n",
    "    1. Construct select statement\n",
    "    2. Retrieve all section_texts associated with article_id\n",
    "    3. Join section_texts into a single string (article_text)\n",
    "    4. Tokenize article_text\n",
    "    5. Return list of tokens\n",
    "    \n",
    "    Input: 100\n",
    "    Output: ['the','austroasiatic','languages','in',...]\n",
    "    '''\n",
    "    select = \"SELECT section_text FROM articles WHERE article_id = \" + str(article_id)\n",
    "\n",
    "    # # Execute the query with the article_id as a parameter\n",
    "    # article = spark.sql(select, article_id).collect()\n",
    "    docs, _ = get_query(select)\n",
    "    \n",
    "    docs = [doc[0] for doc in docs]\n",
    "    doc = '\\n'.join(docs)\n",
    "    \n",
    "    tokens = tokenize(doc)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bulk_articles(article_ids):\n",
    "    corpus = []\n",
    "    for article_id in article_ids:\n",
    "        article = get_article(article_id)\n",
    "        output = (article_id, article)\n",
    "        corpus.append(output)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = '''select distinct article_id from articles'''\n",
    "article_ids, _ = get_query(select)\n",
    "article_ids = [article_id[0] for article_id in article_ids]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orignal Pulling of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_articles = 500000\n",
    "# random_article_ids = random.sample(range(1, 4902648), num_articles)\n",
    "# corpus = get_bulk_articles(random_article_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('corpus-500000.pkl', 'wb') as file:\n",
    "#     pickle.dump(corpus, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('corpus-500000.pkl', 'rb') as file:\n",
    "    corpus = pickle.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PreProcessing Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Spark Packages\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Word2Vec\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import lower, regexp_replace, trim\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.sql.functions import rand, udf\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp import annotator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# Bert Tokenizer\n",
    "# from transformers import BertTokenizer, BertModel # Hugging Face Package\n",
    "# import torch # PyTorch\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TokenizedToast\")\\\n",
    "    .config(\"spark.driver.memory\",\"28G\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_path = os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, spark_path + \"/bin\")\n",
    "sys.path.insert(0, spark_path + \"/python/pyspark/\")\n",
    "sys.path.insert(0, spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.insert(0, spark_path + \"/python/lib/py4j-0.10.7-src.zip\")\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Vectorization Methods\n",
    "1. TFIDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = spark.createDataFrame(corpus, ['id', 'text'])\n",
    "main_df = main_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='temp')\n",
    "stopwords_remover = StopWordsRemover(inputCol='text', outputCol='temp', stopWords=stopwords)\n",
    "count_vectorizer = CountVectorizer(inputCol='text', outputCol='temp')\n",
    "idf = IDF(inputCol='text', outputCol='temp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence(lda_model, df):\n",
    "    return lda_model.logLikelihood(df), lda_model.logPerplexity(df)\n",
    "\n",
    "def tuning(df, tokenizer, stopwords_remover, count_vectorizer, idf, max_k):\n",
    "    coherence_scores = []\n",
    "    perplexity_scores = []\n",
    "    results = list()\n",
    "\n",
    "    df = tokenizer.transform(df)\n",
    "    df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "    df = stopwords_remover.transform(df)\n",
    "    df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "    model_cv = count_vectorizer.fit(df)\n",
    "    df = model_cv.transform(df)\n",
    "    df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "    model_idf = idf.fit(df)\n",
    "    df = model_idf.transform(df)\n",
    "    df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "    for k in range(2, max_k + 1, 5):\n",
    "        cur_result = dict()\n",
    "        lda = LDA(k=k, maxIter=5, featuresCol='text')\n",
    "        model = lda.fit(df)\n",
    "        coherence, perplexity = calculate_coherence(model, df)\n",
    "        coherence_scores.append(coherence)\n",
    "        perplexity_scores.append(perplexity)\n",
    "\n",
    "        sample_size = 100\n",
    "        sampled_articles_df = df.orderBy(rand()).limit(sample_size)\n",
    "        sampled_articles_df = model.transform(sampled_articles_df).select(\"id\", \"text\", \"topicDistribution\")\n",
    "\n",
    "        get_max_topic_udf = udf(lambda vector: int(vector.argmax()), IntegerType())\n",
    "        sampled_articles_df = sampled_articles_df.withColumn(\"maxTopicIndex\", get_max_topic_udf(\"topicDistribution\"))\n",
    "\n",
    "        # Evaluate the clustering results\n",
    "        evaluator = ClusteringEvaluator(predictionCol=\"maxTopicIndex\", featuresCol=\"text\")\n",
    "\n",
    "        # Calculate silhouette coefficient\n",
    "        silhouette = evaluator.evaluate(sampled_articles_df)\n",
    "        print(\"Silhouette coefficient: \", silhouette)\n",
    "\n",
    "        cur_result['silhouette'] = silhouette\n",
    "        cur_result['coherence'] = coherence\n",
    "        cur_result['perplexity'] = perplexity\n",
    "        cur_result['k'] = k\n",
    "        results.append(cur_result)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette coefficient:  0.06498282576481665\n",
      "Silhouette coefficient:  -0.6644439268936647\n",
      "Silhouette coefficient:  -0.6554911981420182\n",
      "Silhouette coefficient:  -0.5839173521105573\n",
      "Silhouette coefficient:  -0.6947313540517591\n",
      "Silhouette coefficient:  -0.5926663297804388\n",
      "Silhouette coefficient:  -0.6664091380762678\n",
      "Silhouette coefficient:  -0.5638120480353167\n",
      "Silhouette coefficient:  -0.5667703468223168\n",
      "Silhouette coefficient:  -0.5310403825602856\n",
      "[{'silhouette': 0.06498282576481665, 'coherence': -6397237.184257759, 'perplexity': 10.290046065957823, 'k': 2}, {'silhouette': -0.6644439268936647, 'coherence': -6539463.403415318, 'perplexity': 10.518818941617287, 'k': 7}, {'silhouette': -0.6554911981420182, 'coherence': -6792370.124809612, 'perplexity': 10.925622963194234, 'k': 12}, {'silhouette': -0.5839173521105573, 'coherence': -7114858.841212765, 'perplexity': 11.444350603261416, 'k': 17}, {'silhouette': -0.6947313540517591, 'coherence': -7472882.418228105, 'perplexity': 12.02023656685404, 'k': 22}, {'silhouette': -0.5926663297804388, 'coherence': -7866957.635673761, 'perplexity': 12.654112101584372, 'k': 27}, {'silhouette': -0.6664091380762678, 'coherence': -8285099.444112921, 'perplexity': 13.32669908671756, 'k': 32}, {'silhouette': -0.5638120480353167, 'coherence': -8731811.895756248, 'perplexity': 14.045242353638837, 'k': 37}, {'silhouette': -0.5667703468223168, 'coherence': -9197321.744458923, 'perplexity': 14.794021498344463, 'k': 42}, {'silhouette': -0.5310403825602856, 'coherence': -9682867.290165458, 'perplexity': 15.575028343726975, 'k': 47}]\n"
     ]
    }
   ],
   "source": [
    "tuning(main_df, tokenizer, stopwords_remover, count_vectorizer, idf, 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the model for the best k-value\n",
    "The best k-value is 2, but since the results of 2 may be a little lack luster we will also choose the second best(highest) silhouette value which is suprisingly our largest; 47. \n",
    "\n",
    "Note we didn't want to save the models in the loop because we wanted to conserve our ram and storage costs. Running the model twice is much less expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = tokenizer.transform(main_df)\n",
    "df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "df = stopwords_remover.transform(df)\n",
    "df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "model_cv = count_vectorizer.fit(df)\n",
    "df = model_cv.transform(df)\n",
    "df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "model_idf = idf.fit(df)\n",
    "df = model_idf.transform(df)\n",
    "df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "lda = LDA(k=2, maxIter=5, featuresCol='text')\n",
    "model_k2 = lda.fit(df)\n",
    "\n",
    "lda = LDA(k=47, maxIter=5, featuresCol='text')\n",
    "model_k47 = lda.fit(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "def analyze_topics(lda_model, df):\n",
    "    transformed = lda_model.transform(df)\n",
    "    topics = transformed.select('id', 'topicDistribution').rdd.map(lambda row: (row[0], row[1].tolist())).toDF(['id', 'topics'])\n",
    "    df_with_topics = df.join(topics, on='id')\n",
    "\n",
    "    # Perform your post-classification analysis here\n",
    "    # Example: Calculate the dominant topic for each document\n",
    "    dominant_topic_udf = udf(lambda topics: int(np.argmax(topics)), IntegerType())\n",
    "    df_with_topics = df_with_topics.withColumn('dominant_topic', dominant_topic_udf(col('topics')))\n",
    "    df_with_topics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------+\n",
      "|     id|                text|              topics|dominant_topic|\n",
      "+-------+--------------------+--------------------+--------------+\n",
      "|2130509|(30521,[6,10,12,2...|[0.99947665275049...|             0|\n",
      "|2346170|(30521,[4,17,21,2...|[0.75411475097072...|             0|\n",
      "|3456671|(30521,[2,12,19,3...|[0.99241565439553...|             0|\n",
      "|4512412|(30521,[45,71,82,...|[0.35551641805267...|             1|\n",
      "|3067872|(30521,[42,66,81,...|[0.65504208781328...|             0|\n",
      "|1949618|(30521,[2,4,7,8,1...|[0.56952636551823...|             0|\n",
      "|2865107|(30521,[0,2,5,22,...|[0.54015024437450...|             0|\n",
      "|1170827|(30521,[1,2,3,4,5...|[0.99991771923471...|             0|\n",
      "| 403062|(30521,[30,32,37,...|[0.38795922793309...|             1|\n",
      "|1491162|(30521,[4,6,9,32,...|[0.00316051490843...|             1|\n",
      "|4193714|(30521,[5,11,22,2...|[0.33843809587893...|             1|\n",
      "|4885976|(30521,[5,11,12,1...|[0.99784490886379...|             0|\n",
      "|1339847|(30521,[3,5,9,10,...|[0.99831273038332...|             0|\n",
      "|1396734|(30521,[5,6,11,16...|[0.38557367482439...|             1|\n",
      "|3606847|(30521,[27,74,140...|[0.00820000673216...|             1|\n",
      "|4410176|(30521,[3,5,6,7,8...|[0.86235200478191...|             0|\n",
      "|2543505|(30521,[4,5,6,7,1...|[7.31842080365434...|             1|\n",
      "|3332324|(30521,[9,12,44,6...|[0.35047600391713...|             1|\n",
      "|3383081|(30521,[2,4,5,9,1...|[0.44336056811648...|             1|\n",
      "|4723792|(30521,[1,2,4,5,6...|[0.80701403273038...|             0|\n",
      "+-------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyze_topics(model_k2, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------+\n",
      "|     id|                text|              topics|dominant_topic|\n",
      "+-------+--------------------+--------------------+--------------+\n",
      "|2130509|(30521,[6,10,12,2...|[0.99904220574373...|             0|\n",
      "|2346170|(30521,[4,17,21,2...|[2.81598070782669...|            40|\n",
      "|3456671|(30521,[2,12,19,3...|[0.56144770489104...|             0|\n",
      "|4512412|(30521,[45,71,82,...|[1.10368789670043...|            21|\n",
      "|3067872|(30521,[42,66,81,...|[9.51289437854830...|             6|\n",
      "|1949618|(30521,[2,4,7,8,1...|[5.09487263239038...|             6|\n",
      "|2865107|(30521,[0,2,5,22,...|[1.49887802795905...|             1|\n",
      "|1170827|(30521,[1,2,3,4,5...|[1.42543850019460...|            40|\n",
      "| 403062|(30521,[30,32,37,...|[1.16062840638495...|            40|\n",
      "|1491162|(30521,[4,6,9,32,...|[2.29475779028124...|            33|\n",
      "|4193714|(30521,[5,11,22,2...|[7.08835760238747...|             1|\n",
      "|4885976|(30521,[5,11,12,1...|[8.82723722686408...|            36|\n",
      "|1339847|(30521,[3,5,9,10,...|[2.21025426432121...|             3|\n",
      "|1396734|(30521,[5,6,11,16...|[2.83633892881606...|             6|\n",
      "|3606847|(30521,[27,74,140...|[2.87465373937405...|             1|\n",
      "|4410176|(30521,[3,5,6,7,8...|[9.58893989249143...|            11|\n",
      "|2543505|(30521,[4,5,6,7,1...|[1.22297560594777...|            11|\n",
      "|3332324|(30521,[9,12,44,6...|[7.77468500330342...|             1|\n",
      "|3383081|(30521,[2,4,5,9,1...|[8.75679472588967...|             6|\n",
      "|4723792|(30521,[1,2,4,5,6...|[7.4960516419183E...|            40|\n",
      "+-------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyze_topics(model_k47, df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this project we estimated clusters for a subset of wikipedia articles with apache spark and analyzed the best performing cluster. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "*None*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Requirements\n",
    "\n",
    "This final project examines the level of knowledge the students have learned from the course. Course outcomes include querying and exploring data using higher-level tools built on top of a cloud computing platform, applying practical tools for processing massive data sets, and building scalable big data analytical and predictive models. \n",
    "\n",
    "** Marking will be foucsed on both presentation and content.** \n",
    "\n",
    "## Written Presentation Requirements\n",
    "The report will be judged on the basis of visual appearance, grammatical correctness, and quality of writing, as well as its contents. Please make sure that the text of your report is well-structured, using paragraphs, full sentences, and other features of well-written presentation.\n",
    "\n",
    "## Technical Content of the Entire Project:\n",
    "* Is the problem well defined and described thoroughly?\n",
    "* Is the size and complexity of the data set used in this project commensurate with the course?\n",
    "* Does the project uses cloud computing techniques for exploratory data analysis?\n",
    "* Does the project uses cloud computing techniques for building analytical and predictive models?\n",
    "* Does the project cover the key data science activites including data cleaning, data wrangling, visualization, model selection, feature engineering, and model evaluation?\n",
    "* Does the report present the findings well and make clear conclusions?\n",
    "* Overall, what is the rating of this project?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ee6f23f9c4d80c203dea4faa6a5d0c8cffa4ab65b221251ea7227320f350d1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
