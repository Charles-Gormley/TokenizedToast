{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SetUp\n",
    "Importing & Device Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\INFO323\\\\TokenizedToast'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading In Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = 'Data\\enwiki-20170820.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(select, db=db):\n",
    "    '''\n",
    "    1. Connects to SQLite database (db)\n",
    "    2. Executes select statement\n",
    "    3. Return results and column names\n",
    "    \n",
    "    Input: 'select * from analytics limit 2'\n",
    "    Output: ([(1, 2, 3)], ['col_1', 'col_2', 'col_3'])\n",
    "    '''\n",
    "    with sql.connect(db) as conn:\n",
    "        c = conn.cursor()\n",
    "        c.execute(select)\n",
    "        col_names = [str(name[0]).lower() for name in c.description]\n",
    "    return c.fetchall(), col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, lower=True):\n",
    "    '''\n",
    "    1. Strips apostrophes\n",
    "    2. Searches for all alpha tokens (exception for underscore)\n",
    "    3. Return list of tokens\n",
    "\n",
    "    Input: 'The 3 dogs jumped over Scott's tent!'\n",
    "    Output: ['the', 'dogs', 'jumped', 'over', 'scotts', 'tent']\n",
    "    '''\n",
    "    text = re.sub(\"'\", \"\", text)\n",
    "    if lower:\n",
    "        tokens = re.findall('''[a-z_]+''', text.lower())\n",
    "    else:\n",
    "        tokens = re.findall('''[A-Za-z_]''', text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(article_id):\n",
    "    '''\n",
    "    1. Construct select statement\n",
    "    2. Retrieve all section_texts associated with article_id\n",
    "    3. Join section_texts into a single string (article_text)\n",
    "    4. Tokenize article_text\n",
    "    5. Return list of tokens\n",
    "    \n",
    "    Input: 100\n",
    "    Output: ['the','austroasiatic','languages','in',...]\n",
    "    '''\n",
    "    select = \"SELECT section_text FROM articles WHERE article_id = \" + str(article_id)\n",
    "\n",
    "    # # Execute the query with the article_id as a parameter\n",
    "    # article = spark.sql(select, article_id).collect()\n",
    "    docs, _ = get_query(select)\n",
    "    \n",
    "    docs = [doc[0] for doc in docs]\n",
    "    doc = '\\n'.join(docs)\n",
    "    \n",
    "    tokens = tokenize(doc)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bulk_articles(article_ids):\n",
    "    corpus = []\n",
    "    for article_id in article_ids:\n",
    "        article = get_article(article_id)\n",
    "        output = (article_id, article)\n",
    "        corpus.append(output)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = '''select distinct article_id from articles'''\n",
    "article_ids, _ = get_query(select)\n",
    "article_ids = [article_id[0] for article_id in article_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_articles = 500000\n",
    "random_article_ids = random.sample(range(1, 4902648), num_articles)\n",
    "corpus = get_bulk_articles(random_article_ids)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PreProcessing Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Spark Packages\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Word2Vec\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import lower, regexp_replace, trim\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp import annotator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# Bert Tokenizer\n",
    "# from transformers import BertTokenizer, BertModel # Hugging Face Package\n",
    "# import torch # PyTorch\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TokenizedToast\")\\\n",
    "    .config(\"spark.driver.memory\",\"28G\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_path = os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, spark_path + \"/bin\")\n",
    "sys.path.insert(0, spark_path + \"/python/pyspark/\")\n",
    "sys.path.insert(0, spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.insert(0, spark_path + \"/python/lib/py4j-0.10.7-src.zip\")\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Vectorization Methods\n",
    "1. TFIDF\n",
    "2. Word2Vec\n",
    "3. Doc2Vec\n",
    "4. BERT Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_val = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(corpus, ['id', 'text'])\n",
    "df = df.dropna()\n",
    "main_df = df # How to create copy of dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='temp')\n",
    "df = tokenizer.transform(main_df)\n",
    "df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "stopwords_remover = StopWordsRemover(inputCol='text', outputCol='temp', stopWords=stopwords)\n",
    "df = stopwords_remover.transform(df)\n",
    "df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "count_vectorizer = CountVectorizer(inputCol='text', outputCol='temp')\n",
    "model_cv = count_vectorizer.fit(df)\n",
    "df = model_cv.transform(df)\n",
    "df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "idf = IDF(inputCol='text', outputCol='temp')\n",
    "model_idf = idf.fit(df)\n",
    "df = model_idf.transform(df)\n",
    "df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "lda = LDA(k=k_val, maxIter=5, featuresCol='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tfidf = lda.fit(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='temp')\n",
    "df = tokenizer.transform(main_df)\n",
    "df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "stopwords_remover = StopWordsRemover(inputCol='text', outputCol='temp', stopWords=stopwords)\n",
    "df = stopwords_remover.transform(df)\n",
    "df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "word2vec = Word2Vec(vectorSize=100, windowSize=5, minCount=2, inputCol='text', outputCol='temp')\n",
    "model_w2v = word2vec.fit(df)\n",
    "df = model_w2v.transform(df)\n",
    "df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "lda = LDA(k=k_val, maxIter=5, featuresCol='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word = lda.fit(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='temp')\n",
    "df = tokenizer.transform(main_df)\n",
    "df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "stopwords_remover = StopWordsRemover(inputCol='text', outputCol='temp', stopWords=stopwords)\n",
    "df = stopwords_remover.transform(df)\n",
    "df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "\n",
    "doc2vec = Word2Vec(vectorSize=100, windowSize=5, minCount=2, inputCol='text', outputCol='temp')\n",
    "model_w2v = doc2vec.fit(df)\n",
    "df = model_w2v.transform(df)\n",
    "average_vector_udf = udf(lambda vectors: Vectors.dense(np.mean(vectors, axis=0)), VectorUDT())\n",
    "df = df.withColumn('documentVector', average_vector_udf(col('temp')))\n",
    "df = df.drop('text').withColumnRenamed('temp', 'text')\n",
    "df = df.drop('text').withColumnRenamed('documentVector', 'text')\n",
    "\n",
    "lda = LDA(k=k_val, maxIter=5, featuresCol='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_doc = lda.fit(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ee6f23f9c4d80c203dea4faa6a5d0c8cffa4ab65b221251ea7227320f350d1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
